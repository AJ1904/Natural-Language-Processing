{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR8E8SZaCRzL"
      },
      "source": [
        "# Translation (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQkgSB_tCRzM"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPtDHMyBCRzM"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the following line:\n",
        "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yGHls_iCRzM"
      },
      "source": [
        "You will need to setup git, adapt your email and name in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k36YpO9GCRzN"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"you@example.com\"\n",
        "!git config --global user.name \"Your Name\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeM-GGgrCRzN"
      },
      "source": [
        "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3VNQqkFCRzN"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW1ZavZICRzN"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsmehkMiCRzN",
        "outputId": "30ce5f37-b0a7-4976-eed7-f29cc368cbd4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'translation'],\n",
              "        num_rows: 210173\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs4VK57tCRzO",
        "outputId": "414f3f5c-770e-42d3-85d8-f0758407dcfb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'translation'],\n",
              "        num_rows: 189155\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'translation'],\n",
              "        num_rows: 21018\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
        "split_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcD4GqfBCRzO"
      },
      "outputs": [],
      "source": [
        "split_datasets[\"validation\"] = split_datasets.pop(\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeapSYyTCRzO",
        "outputId": "3559744a-f5fb-44a3-f0b5-94b1f36fd015"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'en': 'Default to expanded threads',\n",
              " 'fr': 'Par défaut, développer les fils de discussion'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_datasets[\"train\"][1][\"translation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzpLxOF_CRzO",
        "outputId": "fd1b10ad-d900-42cf-ea33-0e0564d30a0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'translation_text': 'Par défaut pour les threads élargis'}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "translator = pipeline(\"translation\", model=model_checkpoint)\n",
        "translator(\"Default to expanded threads\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbqUpYC1CRzO",
        "outputId": "fc39ff50-811a-4205-a57f-d3e26cfaee0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',\n",
              " 'fr': \"Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct.\"}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_datasets[\"train\"][172][\"translation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZ1UfzevCRzO",
        "outputId": "37fe3280-c9ed-4911-ab9f-2890e936629a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'translation_text': \"Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format.\"}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translator(\n",
        "    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kq34TKjACRzO"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li1IcJrHCRzP",
        "outputId": "d2ccecf2-4966-4d57-e4c7-13d347c87f24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en_sentence = split_datasets[\"train\"][1][\"translation\"][\"en\"]\n",
        "fr_sentence = split_datasets[\"train\"][1][\"translation\"][\"fr\"]\n",
        "\n",
        "inputs = tokenizer(en_sentence, text_target=fr_sentence)\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc50oEBCCRzP",
        "outputId": "47bfc7ef-d7a3-4768-e093-985b4b716e60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['▁Par', '▁dé', 'f', 'aut', ',', '▁dé', 've', 'lop', 'per', '▁les', '▁fil', 's', '▁de', '▁discussion', '</s>']\n",
              "['▁Par', '▁défaut', ',', '▁développer', '▁les', '▁fils', '▁de', '▁discussion', '</s>']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wrong_targets = tokenizer(fr_sentence)\n",
        "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
        "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK7n2mzlCRzP"
      },
      "outputs": [],
      "source": [
        "max_length = 128\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
        "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(\n",
        "        inputs, text_target=targets, max_length=max_length, truncation=True\n",
        "    )\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAd5iGi3CRzP"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = split_datasets.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=split_datasets[\"train\"].column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orgp1lR5CRzP"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyOWgXdvCRzP"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHT1F4UBCRzP",
        "outputId": "30d3ac4e-42f0-4720-e932-2f4cc91b5332"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids'])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
        "batch.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IOpVG2BCRzP",
        "outputId": "5d8c27b2-438b-411a-8c99-e21d71b858f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,\n",
              "          -100,  -100,  -100,  -100,  -100,  -100],\n",
              "        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,\n",
              "           550,  7032,  5821,  7907, 12649,     0]])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch[\"labels\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vd3qxzsGCRzP",
        "outputId": "6ae73faf-b49d-47b5-ae81-3a6bb6f77f77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,\n",
              "         59513, 59513, 59513, 59513, 59513, 59513],\n",
              "        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n",
              "           817,   550,  7032,  5821,  7907, 12649]])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch[\"decoder_input_ids\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vtc6jAibCRzP",
        "outputId": "f742de65-1872-455f-fd07-83d83b33191e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]\n",
              "[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for i in range(1, 3):\n",
        "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcEx3RrxCRzP"
      },
      "outputs": [],
      "source": [
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5ZsGSQlCRzP"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"sacrebleu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3NfoOpECRzP",
        "outputId": "6079aea4-21de-4d43-ac53-42dbb5a391c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 46.750469682990165,\n",
              " 'counts': [11, 6, 4, 3],\n",
              " 'totals': [12, 11, 10, 9],\n",
              " 'precisions': [91.67, 54.54, 40.0, 33.33],\n",
              " 'bp': 0.9200444146293233,\n",
              " 'sys_len': 12,\n",
              " 'ref_len': 13}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = [\n",
        "    \"This plugin lets you translate web pages between several languages automatically.\"\n",
        "]\n",
        "references = [\n",
        "    [\n",
        "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
        "    ]\n",
        "]\n",
        "metric.compute(predictions=predictions, references=references)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlfLDmTWCRzQ",
        "outputId": "01e2e7d9-a3a3-42ec-e8f3-bbcc1064c6de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 1.683602693167689,\n",
              " 'counts': [1, 0, 0, 0],\n",
              " 'totals': [4, 3, 2, 1],\n",
              " 'precisions': [25.0, 16.67, 12.5, 12.5],\n",
              " 'bp': 0.10539922456186433,\n",
              " 'sys_len': 4,\n",
              " 'ref_len': 13}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = [\"This This This This\"]\n",
        "references = [\n",
        "    [\n",
        "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
        "    ]\n",
        "]\n",
        "metric.compute(predictions=predictions, references=references)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzwSVcY_CRzQ",
        "outputId": "300db275-98bb-483b-c73a-1e73f06c63dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 0.0,\n",
              " 'counts': [2, 1, 0, 0],\n",
              " 'totals': [2, 1, 0, 0],\n",
              " 'precisions': [100.0, 100.0, 0.0, 0.0],\n",
              " 'bp': 0.004086771438464067,\n",
              " 'sys_len': 2,\n",
              " 'ref_len': 13}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = [\"This plugin\"]\n",
        "references = [\n",
        "    [\n",
        "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
        "    ]\n",
        "]\n",
        "metric.compute(predictions=predictions, references=references)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQYXIQiYCRzQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    # In case the model returns more than the prediction logits\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100s in the labels as we can't decode them\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return {\"bleu\": result[\"score\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfd1l0dxCRzQ"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ2S3r07CRzQ"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"marian-finetuned-kde4-en-to-fr\",\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dkjQiU-CRzQ"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztEMZ-57CRzQ",
        "outputId": "5c24606b-103e-4086-da03-dcfe02767f01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 1.6964408159255981,\n",
              " 'eval_bleu': 39.26865061007616,\n",
              " 'eval_runtime': 965.8884,\n",
              " 'eval_samples_per_second': 21.76,\n",
              " 'eval_steps_per_second': 0.341}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate(max_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSoySvKrCRzQ"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9uKVuLGCRzQ",
        "outputId": "721a738a-7020-4a56-d371-400a6def786d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.8558505773544312,\n",
              " 'eval_bleu': 52.94161337775576,\n",
              " 'eval_runtime': 714.2576,\n",
              " 'eval_samples_per_second': 29.426,\n",
              " 'eval_steps_per_second': 0.461,\n",
              " 'epoch': 3.0}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate(max_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULHhTolTCRzQ",
        "outputId": "d71854d2-0fe6-407e-ccf6-e66cd5e8c1c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/sgugger/marian-finetuned-kde4-en-to-fr/commit/3601d621e3baae2bc63d3311452535f8f58f6ef3'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.push_to_hub(tags=\"translation\", commit_message=\"Training complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OlsC1LYCRzU"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8,\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtH7UhwjCRzU"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IABYIyGvCRzU"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfn_pEaaCRzU"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator()\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taYunP9BCRzU"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_train_epochs = 3\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iDfoHHBCRzU",
        "outputId": "9f0917e5-ec1e-465e-a71a-f52d13905cab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'sgugger/marian-finetuned-kde4-en-to-fr-accelerate'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import Repository, get_full_repo_name\n",
        "\n",
        "model_name = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
        "repo_name = get_full_repo_name(model_name)\n",
        "repo_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-M2S_B-CRzU"
      },
      "outputs": [],
      "source": [
        "output_dir = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
        "repo = Repository(output_dir, clone_from=repo_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nF5KP_4zCRzU"
      },
      "outputs": [],
      "source": [
        "def postprocess(predictions, labels):\n",
        "    predictions = predictions.cpu().numpy()\n",
        "    labels = labels.cpu().numpy()\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
        "    return decoded_preds, decoded_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWtvj4ZDCRzV",
        "outputId": "cada286c-da64-41cd-ed75-1520e75f2fb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "epoch 0, BLEU score: 53.47\n",
              "epoch 1, BLEU score: 54.24\n",
              "epoch 2, BLEU score: 54.44"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    for batch in tqdm(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
        "                batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                max_length=128,\n",
        "            )\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        # Necessary to pad predictions and labels for being gathered\n",
        "        generated_tokens = accelerator.pad_across_processes(\n",
        "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
        "\n",
        "        predictions_gathered = accelerator.gather(generated_tokens)\n",
        "        labels_gathered = accelerator.gather(labels)\n",
        "\n",
        "        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
        "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    results = metric.compute()\n",
        "    print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
        "\n",
        "    # Save and upload\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
        "    if accelerator.is_main_process:\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        repo.push_to_hub(\n",
        "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jI7uTFPCRzV",
        "outputId": "abaca956-43c5-4e68-b6bb-5c9260c70678"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'translation_text': 'Par défaut, développer les fils de discussion'}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Replace this with your own checkpoint\n",
        "model_checkpoint = \"huggingface-course/marian-finetuned-kde4-en-to-fr\"\n",
        "translator = pipeline(\"translation\", model=model_checkpoint)\n",
        "translator(\"Default to expanded threads\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZLIPwYxCRzV",
        "outputId": "f07ee9d8-e6a8-41d7-dd04-faa862c8121d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'translation_text': \"Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format.\"}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translator(\n",
        "    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",
        ")"
      ]
    }
  ]
}