{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDCRefSK8L5Q"
      },
      "source": [
        "# What if my dataset isn't on the Hub?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULoJ9Txq8L5R"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8LchoJr8L5R"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd-bMg2V8L5S"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
        "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9vfyCmC8L5S"
      },
      "outputs": [],
      "source": [
        "!gzip -dkv SQuAD_it-*.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoFssN3v8L5S"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=\"SQuAD_it-train.json\", field=\"data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62WhcjKl8L5S",
        "outputId": "8d783ecb-d1a5-4f3d-bdcb-1755b99d67c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['title', 'paragraphs'],\n",
              "        num_rows: 442\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "squad_it_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bai_6RI_8L5T",
        "outputId": "268a72db-c6d1-42ca-a319-dd6c45cf74b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\n",
              "    \"title\": \"Terremoto del Sichuan del 2008\",\n",
              "    \"paragraphs\": [\n",
              "        {\n",
              "            \"context\": \"Il terremoto del Sichuan del 2008 o il terremoto...\",\n",
              "            \"qas\": [\n",
              "                {\n",
              "                    \"answers\": [{\"answer_start\": 29, \"text\": \"2008\"}],\n",
              "                    \"id\": \"56cdca7862d2951400fa6826\",\n",
              "                    \"question\": \"In quale anno si Ã¨ verificato il terremoto nel Sichuan?\",\n",
              "                },\n",
              "                ...\n",
              "            ],\n",
              "        },\n",
              "        ...\n",
              "    ],\n",
              "}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "squad_it_dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNsC6DKr8L5T",
        "outputId": "68367d43-0fae-42d4-e0d6-40b65b3c1257"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['title', 'paragraphs'],\n",
              "        num_rows: 442\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['title', 'paragraphs'],\n",
              "        num_rows: 48\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_files = {\"train\": \"SQuAD_it-train.json\", \"test\": \"SQuAD_it-test.json\"}\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
        "squad_it_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjI2NiXY8L5T"
      },
      "outputs": [],
      "source": [
        "data_files = {\"train\": \"SQuAD_it-train.json.gz\", \"test\": \"SQuAD_it-test.json.gz\"}\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_AR2RFr8L5T"
      },
      "outputs": [],
      "source": [
        "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
        "data_files = {\n",
        "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
        "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
        "}\n",
        "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYSo2f6y8hXV"
      },
      "source": [
        "# Time to slice and dice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGEsA-rl8hXW"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvqDAyxp8hXW"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zzqmii2G8hXX"
      },
      "outputs": [],
      "source": [
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
        "!unzip drugsCom_raw.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Al-27SXV8hXX"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data_files = {\"train\": \"drugsComTrain_raw.tsv\", \"test\": \"drugsComTest_raw.tsv\"}\n",
        "# \\t is the tab character in Python\n",
        "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpOThYp38hXX",
        "outputId": "02bce352-c590-4127-e5d7-a0af9076e9a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Unnamed: 0': [87571, 178045, 80482],\n",
              " 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],\n",
              " 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],\n",
              " 'review': ['\"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!\"',\n",
              "  '\"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\\r\\nas a pain reducer and an anti-depressant, however, the side effects outweighed \\r\\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\\r\\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\\r\\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\\r\\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"',\n",
              "  '\"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.\"'],\n",
              " 'rating': [9.0, 3.0, 10.0],\n",
              " 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],\n",
              " 'usefulCount': [36, 13, 128]}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "# Peek at the first few examples\n",
        "drug_sample[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ86j4AJ8hXX"
      },
      "outputs": [],
      "source": [
        "for split in drug_dataset.keys():\n",
        "    assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\"Unnamed: 0\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83NCSwkX8hXX",
        "outputId": "bac1608f-803a-41e2-e55c-6d2ec06adcd1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
              "        num_rows: 161297\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
              "        num_rows: 53766\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "drug_dataset = drug_dataset.rename_column(\n",
        "    original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\"\n",
        ")\n",
        "drug_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fqAk7KQ8hXY",
        "outputId": "5d9439bb-8fc8-4a52-c095-ff5ce98b32a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AttributeError: 'NoneType' object has no attribute 'lower'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def lowercase_condition(example):\n",
        "    return {\"condition\": example[\"condition\"].lower()}\n",
        "\n",
        "\n",
        "drug_dataset.map(lowercase_condition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnkGv5Bh8hXY"
      },
      "outputs": [],
      "source": [
        "def filter_nones(x):\n",
        "    return x[\"condition\"] is not None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtO3AvGJ8hXY",
        "outputId": "88c0db47-a1db-419d-b4c5-4f0f1666cddd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(lambda x: x * x)(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nob0JuLT8hXY",
        "outputId": "a2dfe35d-040a-45f0-c199-677ea22288ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16.0"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(lambda base, height: 0.5 * base * height)(4, 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0z0JIgi8hXY"
      },
      "outputs": [],
      "source": [
        "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLBPTL438hXY",
        "outputId": "2fb71e41-b943-4f82-a41e-08c740c4b72a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['left ventricular dysfunction', 'adhd', 'birth control']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "drug_dataset = drug_dataset.map(lowercase_condition)\n",
        "# Check that lowercasing worked\n",
        "drug_dataset[\"train\"][\"condition\"][:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SX09DEHY8hXY"
      },
      "outputs": [],
      "source": [
        "def compute_review_length(example):\n",
        "    return {\"review_length\": len(example[\"review\"].split())}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OlFlDA_8hXY",
        "outputId": "3574f510-573a-4427-a05b-49df807d788f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'patient_id': 206461,\n",
              " 'drugName': 'Valsartan',\n",
              " 'condition': 'left ventricular dysfunction',\n",
              " 'review': '\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"',\n",
              " 'rating': 9.0,\n",
              " 'date': 'May 20, 2012',\n",
              " 'usefulCount': 27,\n",
              " 'review_length': 17}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "drug_dataset = drug_dataset.map(compute_review_length)\n",
        "# Inspect the first training example\n",
        "drug_dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF7orrkd8hXY",
        "outputId": "67c077d0-1bd7-4ae9-90a9-ff3cdaf1f530"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'patient_id': [103488, 23627, 20558],\n",
              " 'drugName': ['Loestrin 21 1 / 20', 'Chlorzoxazone', 'Nucynta'],\n",
              " 'condition': ['birth control', 'muscle spasm', 'pain'],\n",
              " 'review': ['\"Excellent.\"', '\"useless\"', '\"ok\"'],\n",
              " 'rating': [10.0, 1.0, 6.0],\n",
              " 'date': ['November 4, 2008', 'March 24, 2017', 'August 20, 2016'],\n",
              " 'usefulCount': [5, 2, 10],\n",
              " 'review_length': [1, 1, 1]}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "drug_dataset[\"train\"].sort(\"review_length\")[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc6ps8-B8hXZ",
        "outputId": "68d8b3c7-acb0-466c-f252-0702421c795d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'train': 138514, 'test': 46108}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)\n",
        "print(drug_dataset.num_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnz7bbIf8hXZ",
        "outputId": "08b52f06-6fb1-460d-e85e-63807473150f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I'm a transformer called BERT\""
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import html\n",
        "\n",
        "text = \"I&#039;m a transformer called BERT\"\n",
        "html.unescape(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JlVyhhv8hXZ"
      },
      "outputs": [],
      "source": [
        "drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXZEfnFM8hXZ"
      },
      "outputs": [],
      "source": [
        "new_drug_dataset = drug_dataset.map(\n",
        "    lambda x: {\"review\": [html.unescape(o) for o in x[\"review\"]]}, batched=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ13YwB68hXZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"review\"], truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUeMtsG-8hXZ"
      },
      "outputs": [],
      "source": [
        "%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AGBKIsN8hXZ"
      },
      "outputs": [],
      "source": [
        "slow_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=False)\n",
        "\n",
        "\n",
        "def slow_tokenize_function(examples):\n",
        "    return slow_tokenizer(examples[\"review\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epaUi-pl8hXZ"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_split(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"review\"],\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_overflowing_tokens=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPFOViTE8hXZ",
        "outputId": "3d4720ba-79cb-47ba-b5c4-5a6e46d377bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[128, 49]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = tokenize_and_split(drug_dataset[\"train\"][0])\n",
        "[len(inp) for inp in result[\"input_ids\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPLBqx1e8hXZ",
        "outputId": "f6cde567-9421-4ed2-86c7-d3b4bc52ef50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ArrowInvalid: Column 1 named condition expected length 1463 but got length 1000"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwaJa1r48hXZ"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = drug_dataset.map(\n",
        "    tokenize_and_split, batched=True, remove_columns=drug_dataset[\"train\"].column_names\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZDI0UG68hXZ",
        "outputId": "fa685108-df59-4162-80a7-7564c2e506a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(206772, 138514)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenized_dataset[\"train\"]), len(drug_dataset[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVcQDIhW8hXZ"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_split(examples):\n",
        "    result = tokenizer(\n",
        "        examples[\"review\"],\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_overflowing_tokens=True,\n",
        "    )\n",
        "    # Extract mapping between new and old indices\n",
        "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
        "    for key, values in examples.items():\n",
        "        result[key] = [values[i] for i in sample_map]\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWKpDm3G8hXZ",
        "outputId": "a6cd3d12-8069-4064-c518-88e8525bc1a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],\n",
              "        num_rows: 206772\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],\n",
              "        num_rows: 68876\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)\n",
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7Iwe5wb8hXZ"
      },
      "outputs": [],
      "source": [
        "drug_dataset.set_format(\"pandas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTN5t-S48hXZ"
      },
      "outputs": [],
      "source": [
        "drug_dataset[\"train\"][:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_HyTQd18hXZ"
      },
      "outputs": [],
      "source": [
        "train_df = drug_dataset[\"train\"][:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwf5WkeZ8hXZ"
      },
      "outputs": [],
      "source": [
        "frequencies = (\n",
        "    train_df[\"condition\"]\n",
        "    .value_counts()\n",
        "    .to_frame()\n",
        "    .reset_index()\n",
        "    .rename(columns={\"index\": \"condition\", \"condition\": \"frequency\"})\n",
        ")\n",
        "frequencies.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEJuy6Gq8hXZ",
        "outputId": "deff7bd9-aa10-451b-a061-cc47820031da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['condition', 'frequency'],\n",
              "    num_rows: 819\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "freq_dataset = Dataset.from_pandas(frequencies)\n",
        "freq_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtHtpJBH8hXa"
      },
      "outputs": [],
      "source": [
        "drug_dataset.reset_format()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydkOILTc8hXa",
        "outputId": "89005585-34e0-4dd3-8a2c-c38a0d05e024"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],\n",
              "        num_rows: 110811\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],\n",
              "        num_rows: 27703\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],\n",
              "        num_rows: 46108\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
        "# Rename the default \"test\" split to \"validation\"\n",
        "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
        "# Add the \"test\" set to our `DatasetDict`\n",
        "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
        "drug_dataset_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5hSDilx8hXa"
      },
      "outputs": [],
      "source": [
        "drug_dataset_clean.save_to_disk(\"drug-reviews\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gqqm0r_8hXe",
        "outputId": "5197e3bb-1763-4091-f600-d51e923aa7fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
              "        num_rows: 110811\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
              "        num_rows: 27703\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
              "        num_rows: 46108\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "drug_dataset_reloaded = load_from_disk(\"drug-reviews\")\n",
        "drug_dataset_reloaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTkS1YDP8hXe"
      },
      "outputs": [],
      "source": [
        "for split, dataset in drug_dataset_clean.items():\n",
        "    dataset.to_json(f\"drug-reviews-{split}.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlSHJ3Lv8hXe",
        "outputId": "de6f5a98-56b7-4d58-a46c-fd42a986d2cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\"patient_id\":141780,\"drugName\":\"Escitalopram\",\"condition\":\"depression\",\"review\":\"\\\"I seemed to experience the regular side effects of LEXAPRO, insomnia, low sex drive, sleepiness during the day. I am taking it at night because my doctor said if it made me tired to take it at night. I assumed it would and started out taking it at night. Strange dreams, some pleasant. I was diagnosed with fibromyalgia. Seems to be helping with the pain. Have had anxiety and depression in my family, and have tried quite a few other medications that haven't worked. Only have been on it for two weeks but feel more positive in my mind, want to accomplish more in my life. Hopefully the side effects will dwindle away, worth it to stick with it from hearing others responses. Great medication.\\\"\",\"rating\":9.0,\"date\":\"May 29, 2011\",\"usefulCount\":10,\"review_length\":125}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!head -n 1 drug-reviews-train.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTXTxNI-8hXe"
      },
      "outputs": [],
      "source": [
        "data_files = {\n",
        "    \"train\": \"drug-reviews-train.jsonl\",\n",
        "    \"validation\": \"drug-reviews-validation.jsonl\",\n",
        "    \"test\": \"drug-reviews-test.jsonl\",\n",
        "}\n",
        "drug_dataset_reloaded = load_dataset(\"json\", data_files=data_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQkysB5v8o1L"
      },
      "source": [
        "# Big data? ð¤ Datasets to the rescue!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWyWxWAV8o1M"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8XX-YdQ8o1N"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTF16J8X8o1N"
      },
      "outputs": [],
      "source": [
        "!pip install zstandard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MFJvOiA8o1N",
        "outputId": "aa938583-7a02-482f-9e77-cc4b73f9c67e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['meta', 'text'],\n",
              "    num_rows: 15518009\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# This takes a few minutes to run, so go grab a tea or coffee while you wait :)\n",
        "data_files = \"https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\n",
        "pubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n",
        "pubmed_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Daj7-tXl8o1O",
        "outputId": "10eb5e58-ae53-48fd-8f76-17dc29742993"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'meta': {'pmid': 11409574, 'language': 'eng'},\n",
              " 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pubmed_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwOLn8Zo8o1O"
      },
      "outputs": [],
      "source": [
        "!pip install psutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS7hgBeN8o1O",
        "outputId": "6f525a7a-f470-421f-916a-5cc4467d223f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RAM used: 5678.33 MB"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import psutil\n",
        "\n",
        "# Process.memory_info is expressed in bytes, so convert to megabytes\n",
        "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbeYO_M98o1O",
        "outputId": "2acffc81-1eca-4131-af8d-4065e147fb20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Number of files in dataset : 20979437051\n",
              "Dataset size (cache file) : 19.54 GB"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"Number of files in dataset : {pubmed_dataset.dataset_size}\")\n",
        "size_gb = pubmed_dataset.dataset_size / (1024**3)\n",
        "print(f\"Dataset size (cache file) : {size_gb:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYtZxYm58o1O",
        "outputId": "bbbffb1b-dbae-49a6-e3a0-d7bec71bbb6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import timeit\n",
        "\n",
        "code_snippet = \"\"\"batch_size = 1000\n",
        "\n",
        "for idx in range(0, len(pubmed_dataset), batch_size):\n",
        "    _ = pubmed_dataset[idx:idx + batch_size]\n",
        "\"\"\"\n",
        "\n",
        "time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())\n",
        "print(\n",
        "    f\"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in \"\n",
        "    f\"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAFwIChO8o1P"
      },
      "outputs": [],
      "source": [
        "pubmed_dataset_streamed = load_dataset(\n",
        "    \"json\", data_files=data_files, split=\"train\", streaming=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "le4L3xLR8o1P",
        "outputId": "517dbd6c-3966-44d8-b6f7-ec66fc5260c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'meta': {'pmid': 11409574, 'language': 'eng'},\n",
              " 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(pubmed_dataset_streamed))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "090KtYA98o1P",
        "outputId": "c7893286-cbbd-4f3e-9590-30e90e0afd55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 4958, 5178, 4328, 6779, ...], 'attention_mask': [1, 1, 1, 1, 1, ...]}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x[\"text\"]))\n",
        "next(iter(tokenized_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7ivVtZm8o1P",
        "outputId": "b95f9873-b691-4fa5-c5ff-fa9632434857"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'meta': {'pmid': 11410799, 'language': 'eng'},\n",
              " 'text': 'Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)\n",
        "next(iter(shuffled_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4t0fgrkK8o1P",
        "outputId": "e33db214-a14e-4076-9d6c-882972189f78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'meta': {'pmid': 11409574, 'language': 'eng'},\n",
              "  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},\n",
              " {'meta': {'pmid': 11409575, 'language': 'eng'},\n",
              "  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'},\n",
              " {'meta': {'pmid': 11409576, 'language': 'eng'},\n",
              "  'text': \"Hypoxaemia in children with severe pneumonia in Papua New Guinea ...\"},\n",
              " {'meta': {'pmid': 11409577, 'language': 'eng'},\n",
              "  'text': 'Oxygen concentrators and cylinders ...'},\n",
              " {'meta': {'pmid': 11409578, 'language': 'eng'},\n",
              "  'text': 'Oxygen supply in rural africa: a personal experience ...'}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_head = pubmed_dataset_streamed.take(5)\n",
        "list(dataset_head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqPu_Fxj8o1P"
      },
      "outputs": [],
      "source": [
        "# Skip the first 1,000 examples and include the rest in the training set\n",
        "train_dataset = shuffled_dataset.skip(1000)\n",
        "# Take the first 1,000 examples for the validation set\n",
        "validation_dataset = shuffled_dataset.take(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZa4TiBp8o1P",
        "outputId": "68b72de5-6dbd-4346-9e49-310a8ae2a4bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'meta': {'case_ID': '110921.json',\n",
              "  'case_jurisdiction': 'scotus.tar.gz',\n",
              "  'date_created': '2010-04-28T17:12:49Z'},\n",
              " 'text': '\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "law_dataset_streamed = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst\",\n",
        "    split=\"train\",\n",
        "    streaming=True,\n",
        ")\n",
        "next(iter(law_dataset_streamed))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ar4hkw278o1P",
        "outputId": "7c75575e-3e54-42df-e49d-caed7e886da4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'meta': {'pmid': 11409574, 'language': 'eng'},\n",
              "  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},\n",
              " {'meta': {'case_ID': '110921.json',\n",
              "   'case_jurisdiction': 'scotus.tar.gz',\n",
              "   'date_created': '2010-04-28T17:12:49Z'},\n",
              "  'text': '\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from itertools import islice\n",
        "from datasets import interleave_datasets\n",
        "\n",
        "combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])\n",
        "list(islice(combined_dataset, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJ-iU3yt8o1P",
        "outputId": "19eac458-3c0b-4832-f208-d8729c70d57c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'meta': {'pile_set_name': 'Pile-CC'},\n",
              " 'text': 'It is done, and submitted. You can play âSurvival of the Tastiestâ on Android, and on the web...'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_url = \"https://the-eye.eu/public/AI/pile/\"\n",
        "data_files = {\n",
        "    \"train\": [base_url + \"train/\" + f\"{idx:02d}.jsonl.zst\" for idx in range(30)],\n",
        "    \"validation\": base_url + \"val.jsonl.zst\",\n",
        "    \"test\": base_url + \"test.jsonl.zst\",\n",
        "}\n",
        "pile_dataset = load_dataset(\"json\", data_files=data_files, streaming=True)\n",
        "next(iter(pile_dataset[\"train\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaQaao5c8taW"
      },
      "source": [
        "# Creating your own dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQcWVP5M8taX"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Vhqwhg98taY"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo4RhrZd8taY"
      },
      "source": [
        "You will need to setup git, adapt your email and name in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLhTp96r8taY"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"you@example.com\"\n",
        "!git config --global user.name \"Your Name\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuEMbb2e8taY"
      },
      "source": [
        "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LoseK1a8taZ"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYTRF-lh8taZ"
      },
      "outputs": [],
      "source": [
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDz9nBf_8taZ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1\"\n",
        "response = requests.get(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7iL9Ar88taZ",
        "outputId": "2906a137-70b0-4818-e536-72782634e346"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.status_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YGtls9f8taZ",
        "outputId": "6f6c941a-291c-49b6-b36f-085277e7bbe4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',\n",
              "  'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
              "  'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792/comments',\n",
              "  'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792/events',\n",
              "  'html_url': 'https://github.com/huggingface/datasets/pull/2792',\n",
              "  'id': 968650274,\n",
              "  'node_id': 'MDExOlB1bGxSZXF1ZXN0NzEwNzUyMjc0',\n",
              "  'number': 2792,\n",
              "  'title': 'Update GooAQ',\n",
              "  'user': {'login': 'bhavitvyamalik',\n",
              "   'id': 19718818,\n",
              "   'node_id': 'MDQ6VXNlcjE5NzE4ODE4',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/bhavitvyamalik',\n",
              "   'html_url': 'https://github.com/bhavitvyamalik',\n",
              "   'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',\n",
              "   'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',\n",
              "   'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',\n",
              "   'type': 'User',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 1,\n",
              "  'created_at': '2021-08-12T11:40:18Z',\n",
              "  'updated_at': '2021-08-12T12:31:17Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'CONTRIBUTOR',\n",
              "  'active_lock_reason': None,\n",
              "  'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/2792',\n",
              "   'html_url': 'https://github.com/huggingface/datasets/pull/2792',\n",
              "   'diff_url': 'https://github.com/huggingface/datasets/pull/2792.diff',\n",
              "   'patch_url': 'https://github.com/huggingface/datasets/pull/2792.patch'},\n",
              "  'body': '[GooAQ](https://github.com/allenai/gooaq) dataset was recently updated after splits were added for the same. This PR contains new updated GooAQ with train/val/test splits and updated README as well.',\n",
              "  'performed_via_github_app': None}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnJAABYW8taa"
      },
      "outputs": [],
      "source": [
        "GITHUB_TOKEN = xxx  # Copy your GitHub token here\n",
        "headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpGcyFxW8taa"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "def fetch_issues(\n",
        "    owner=\"huggingface\",\n",
        "    repo=\"datasets\",\n",
        "    num_issues=10_000,\n",
        "    rate_limit=5_000,\n",
        "    issues_path=Path(\".\"),\n",
        "):\n",
        "    if not issues_path.is_dir():\n",
        "        issues_path.mkdir(exist_ok=True)\n",
        "\n",
        "    batch = []\n",
        "    all_issues = []\n",
        "    per_page = 100  # Number of issues to return per page\n",
        "    num_pages = math.ceil(num_issues / per_page)\n",
        "    base_url = \"https://api.github.com/repos\"\n",
        "\n",
        "    for page in tqdm(range(num_pages)):\n",
        "        # Query with state=all to get both open and closed issues\n",
        "        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
        "        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n",
        "        batch.extend(issues.json())\n",
        "\n",
        "        if len(batch) > rate_limit and len(all_issues) < num_issues:\n",
        "            all_issues.extend(batch)\n",
        "            batch = []  # Flush batch for next time period\n",
        "            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n",
        "            time.sleep(60 * 60 + 1)\n",
        "\n",
        "    all_issues.extend(batch)\n",
        "    df = pd.DataFrame.from_records(all_issues)\n",
        "    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n",
        "    print(\n",
        "        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srj50RMY8taa"
      },
      "outputs": [],
      "source": [
        "# Depending on your internet connection, this can take several minutes to run...\n",
        "fetch_issues()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwL24ejd8taa",
        "outputId": "d6ee4390-8e49-47f4-f576-a266d5343bcf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app'],\n",
              "    num_rows: 3019\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "issues_dataset = load_dataset(\"json\", data_files=\"datasets-issues.jsonl\", split=\"train\")\n",
        "issues_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nQZRifm8taa",
        "outputId": "922f2663-15a8-4b47-ec83-9b2f85e2927e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              ">> URL: https://github.com/huggingface/datasets/pull/850\n",
              ">> Pull request: {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/850', 'html_url': 'https://github.com/huggingface/datasets/pull/850', 'diff_url': 'https://github.com/huggingface/datasets/pull/850.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/850.patch'}\n",
              "\n",
              ">> URL: https://github.com/huggingface/datasets/issues/2773\n",
              ">> Pull request: None\n",
              "\n",
              ">> URL: https://github.com/huggingface/datasets/pull/783\n",
              ">> Pull request: {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/783', 'html_url': 'https://github.com/huggingface/datasets/pull/783', 'diff_url': 'https://github.com/huggingface/datasets/pull/783.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/783.patch'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample = issues_dataset.shuffle(seed=666).select(range(3))\n",
        "\n",
        "# Print out the URL and pull request entries\n",
        "for url, pr in zip(sample[\"html_url\"], sample[\"pull_request\"]):\n",
        "    print(f\">> URL: {url}\")\n",
        "    print(f\">> Pull request: {pr}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHul0l6v8taa"
      },
      "outputs": [],
      "source": [
        "issues_dataset = issues_dataset.map(\n",
        "    lambda x: {\"is_pull_request\": False if x[\"pull_request\"] is None else True}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEFiKoDp8taa",
        "outputId": "941f878c-0fa8-4057-8b26-c399208e644b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/897594128',\n",
              "  'html_url': 'https://github.com/huggingface/datasets/pull/2792#issuecomment-897594128',\n",
              "  'issue_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',\n",
              "  'id': 897594128,\n",
              "  'node_id': 'IC_kwDODunzps41gDMQ',\n",
              "  'user': {'login': 'bhavitvyamalik',\n",
              "   'id': 19718818,\n",
              "   'node_id': 'MDQ6VXNlcjE5NzE4ODE4',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/bhavitvyamalik',\n",
              "   'html_url': 'https://github.com/bhavitvyamalik',\n",
              "   'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',\n",
              "   'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',\n",
              "   'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',\n",
              "   'type': 'User',\n",
              "   'site_admin': False},\n",
              "  'created_at': '2021-08-12T12:21:52Z',\n",
              "  'updated_at': '2021-08-12T12:31:17Z',\n",
              "  'author_association': 'CONTRIBUTOR',\n",
              "  'body': \"@albertvillanova my tests are failing here:\\r\\n```\\r\\ndataset_name = 'gooaq'\\r\\n\\r\\n    def test_load_dataset(self, dataset_name):\\r\\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\\r\\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\\r\\n\\r\\ntests/test_dataset_common.py:234: \\r\\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \\r\\ntests/test_dataset_common.py:187: in check_load_dataset\\r\\n    self.parent.assertTrue(len(dataset[split]) > 0)\\r\\nE   AssertionError: False is not true\\r\\n```\\r\\nWhen I try loading dataset on local machine it works fine. Any suggestions on how can I avoid this error?\",\n",
              "  'performed_via_github_app': None}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "issue_number = 2792\n",
        "url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
        "response = requests.get(url, headers=headers)\n",
        "response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqY1WVYV8taa",
        "outputId": "d8d80e4e-8138-4ba5-8365-dfd96c8ecf22"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"@albertvillanova my tests are failing here:\\r\\n```\\r\\ndataset_name = 'gooaq'\\r\\n\\r\\n    def test_load_dataset(self, dataset_name):\\r\\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\\r\\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\\r\\n\\r\\ntests/test_dataset_common.py:234: \\r\\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \\r\\ntests/test_dataset_common.py:187: in check_load_dataset\\r\\n    self.parent.assertTrue(len(dataset[split]) > 0)\\r\\nE   AssertionError: False is not true\\r\\n```\\r\\nWhen I try loading dataset on local machine it works fine. Any suggestions on how can I avoid this error?\"]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_comments(issue_number):\n",
        "    url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
        "    response = requests.get(url, headers=headers)\n",
        "    return [r[\"body\"] for r in response.json()]\n",
        "\n",
        "\n",
        "# Test our function works as expected\n",
        "get_comments(2792)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zD7uUd9o8taa"
      },
      "outputs": [],
      "source": [
        "# Depending on your internet connection, this can take a few minutes...\n",
        "issues_with_comments_dataset = issues_dataset.map(\n",
        "    lambda x: {\"comments\": get_comments(x[\"number\"])}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK-e7RnR8taa"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3SvFeuC8taa"
      },
      "outputs": [],
      "source": [
        "issues_with_comments_dataset.push_to_hub(\"github-issues\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aky6LIoh8taa",
        "outputId": "ccee9a71-49c5-43a9-9f34-6acad79ae583"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],\n",
              "    num_rows: 2855\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "remote_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
        "remote_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SeVBhPsN8yQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Isz0mb9C8woM"
      },
      "source": [
        "# Semantic search with FAISS (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K41aPH4f8woN"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RufilZuI8woN"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7oqxFFG8woO",
        "outputId": "e0202cc1-0ca2-4146-f8d7-017bf011157a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],\n",
              "    num_rows: 2855\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
        "issues_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcsqZLjL8woO",
        "outputId": "488cbbb2-47e3-456a-9551-b209486b41b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],\n",
              "    num_rows: 771\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "issues_dataset = issues_dataset.filter(\n",
        "    lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",
        ")\n",
        "issues_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySUOFd2m8woO",
        "outputId": "086b99bb-256e-4a69-9079-221694ae1dad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body'],\n",
              "    num_rows: 771\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "columns = issues_dataset.column_names\n",
        "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
        "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
        "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
        "issues_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNX5AnIf8woO"
      },
      "outputs": [],
      "source": [
        "issues_dataset.set_format(\"pandas\")\n",
        "df = issues_dataset[:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W8fTBiO8woO",
        "outputId": "88743efb-8dbe-4eef-f5d1-f873fd75d9ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['the bug code locate in ï¼\\r\\n    if data_args.task_name is not None:\\r\\n        # Downloading and loading a dataset from the hub.\\r\\n        datasets = load_dataset(\"glue\", data_args.task_name, cache_dir=model_args.cache_dir)',\n",
              " 'Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com\\r\\n\\r\\nNormally, it should work if you wait a little and then retry.\\r\\n\\r\\nCould you please confirm if the problem persists?',\n",
              " 'cannot connectï¼even by Web browserï¼please check that  there is some  problemsã',\n",
              " 'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"comments\"][0].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_u0GaQ18woP"
      },
      "outputs": [],
      "source": [
        "comments_df = df.explode(\"comments\", ignore_index=True)\n",
        "comments_df.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va9XZlEq8woP",
        "outputId": "d828c25d-4eb4-4649-e5cc-65ec8234f551"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body'],\n",
              "    num_rows: 2842\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "comments_dataset = Dataset.from_pandas(comments_df)\n",
        "comments_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-DCapxn8woP"
      },
      "outputs": [],
      "source": [
        "comments_dataset = comments_dataset.map(\n",
        "    lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBne6HuM8woP",
        "outputId": "c083e8cc-4956-4738-cd73-d8c24454b845"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
              "    num_rows: 2098\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)\n",
        "comments_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nurlxWMx8woP"
      },
      "outputs": [],
      "source": [
        "def concatenate_text(examples):\n",
        "    return {\n",
        "        \"text\": examples[\"title\"]\n",
        "        + \" \\n \"\n",
        "        + examples[\"body\"]\n",
        "        + \" \\n \"\n",
        "        + examples[\"comments\"]\n",
        "    }\n",
        "\n",
        "\n",
        "comments_dataset = comments_dataset.map(concatenate_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Drg3gWQk8woP"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = AutoModel.from_pretrained(model_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXMDkwuq8woP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EsoWWXb8woQ"
      },
      "outputs": [],
      "source": [
        "def cls_pooling(model_output):\n",
        "    return model_output.last_hidden_state[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcPjqVBL8woQ"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(text_list):\n",
        "    encoded_input = tokenizer(\n",
        "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
        "    model_output = model(**encoded_input)\n",
        "    return cls_pooling(model_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZAl6uO88woQ",
        "outputId": "882b912e-881a-4c19-c14f-41be70911038"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
        "embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1QqCiRd8woQ"
      },
      "outputs": [],
      "source": [
        "embeddings_dataset = comments_dataset.map(\n",
        "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NgK7azx8woQ"
      },
      "outputs": [],
      "source": [
        "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUQex2A_8woQ",
        "outputId": "cf436154-658a-4909-f66a-092332332d4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"How can I load a dataset offline?\"\n",
        "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
        "question_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSv2w9jU8woQ"
      },
      "outputs": [],
      "source": [
        "scores, samples = embeddings_dataset.get_nearest_examples(\n",
        "    \"embeddings\", question_embedding, k=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqumuV0z8woQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "samples_df = pd.DataFrame.from_dict(samples)\n",
        "samples_df[\"scores\"] = scores\n",
        "samples_df.sort_values(\"scores\", ascending=False, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_8wtC2U8woQ",
        "outputId": "3dd8e158-dba7-42dc-f043-d58f646905ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\"\"\n",
              "COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\n",
              "\n",
              "@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
              "SCORE: 25.505046844482422\n",
              "TITLE: Discussion using datasets in offline mode\n",
              "URL: https://github.com/huggingface/datasets/issues/824\n",
              "==================================================\n",
              "\n",
              "COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\n",
              "You can now use them offline\n",
              "\\`\\`\\`python\n",
              "datasets = load_dataset(\"text\", data_files=data_files)\n",
              "\\`\\`\\`\n",
              "\n",
              "We'll do a new release soon\n",
              "SCORE: 24.555509567260742\n",
              "TITLE: Discussion using datasets in offline mode\n",
              "URL: https://github.com/huggingface/datasets/issues/824\n",
              "==================================================\n",
              "\n",
              "COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\n",
              "\n",
              "Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :)\n",
              "\n",
              "I already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\n",
              "\n",
              "----------\n",
              "\n",
              "> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
              "\n",
              "Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.\n",
              "For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\n",
              "\\`\\`\\`python\n",
              "load_dataset(\"./my_dataset\")\n",
              "\\`\\`\\`\n",
              "and the dataset script will generate your dataset once and for all.\n",
              "\n",
              "----------\n",
              "\n",
              "About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.\n",
              "cf #1724\n",
              "SCORE: 24.14896583557129\n",
              "TITLE: Discussion using datasets in offline mode\n",
              "URL: https://github.com/huggingface/datasets/issues/824\n",
              "==================================================\n",
              "\n",
              "COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine\n",
              ">\n",
              "> 1. (online machine)\n",
              ">\n",
              "> ```\n",
              ">\n",
              "> import datasets\n",
              ">\n",
              "> data = datasets.load_dataset(...)\n",
              ">\n",
              "> data.save_to_disk(/YOUR/DATASET/DIR)\n",
              ">\n",
              "> ```\n",
              ">\n",
              "> 2. copy the dir from online to the offline machine\n",
              ">\n",
              "> 3. (offline machine)\n",
              ">\n",
              "> ```\n",
              ">\n",
              "> import datasets\n",
              ">\n",
              "> data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
              ">\n",
              "> ```\n",
              ">\n",
              ">\n",
              ">\n",
              "> HTH.\n",
              "\n",
              "\n",
              "SCORE: 22.893993377685547\n",
              "TITLE: Discussion using datasets in offline mode\n",
              "URL: https://github.com/huggingface/datasets/issues/824\n",
              "==================================================\n",
              "\n",
              "COMMENT: here is my way to load a dataset offline, but it **requires** an online machine\n",
              "1. (online machine)\n",
              "\\`\\`\\`\n",
              "import datasets\n",
              "data = datasets.load_dataset(...)\n",
              "data.save_to_disk(/YOUR/DATASET/DIR)\n",
              "\\`\\`\\`\n",
              "2. copy the dir from online to the offline machine\n",
              "3. (offline machine)\n",
              "\\`\\`\\`\n",
              "import datasets\n",
              "data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
              "\\`\\`\\`\n",
              "\n",
              "HTH.\n",
              "SCORE: 22.406635284423828\n",
              "TITLE: Discussion using datasets in offline mode\n",
              "URL: https://github.com/huggingface/datasets/issues/824\n",
              "==================================================\n",
              "\"\"\""
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for _, row in samples_df.iterrows():\n",
        "    print(f\"COMMENT: {row.comments}\")\n",
        "    print(f\"SCORE: {row.scores}\")\n",
        "    print(f\"TITLE: {row.title}\")\n",
        "    print(f\"URL: {row.html_url}\")\n",
        "    print(\"=\" * 50)\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}